## Model Interpretation and Decision Boundary Analysis

### Interpreting Learned Coefficients

After training, the logistic regression model provides a set of learned weights (coefficients) for each input feature, along with a bias term. These coefficients represent the strength and direction of each feature's influence on the predicted probability of the positive class:

- **Positive Coefficient**: Indicates that as the feature value increases, the likelihood of the positive class increases.
- **Negative Coefficient**: Suggests that higher values of the feature decrease the likelihood of the positive class.
- **Magnitude**: The larger the absolute value of a coefficient, the more influential that feature is in the decision-making process.

By examining the printed weights, we can identify which features are most predictive and how they contribute to the classification boundary.

---

### Understanding the Decision Boundary

In logistic regression, the decision boundary is defined by the equation:

\[
\mathbf{w} \cdot \mathbf{x} + b = 0
\]

Where:
- \(\mathbf{w}\) is the vector of learned weights,
- \(\mathbf{x}\) is the input feature vector,
- \(b\) is the bias term.

This equation defines a hyperplane in the feature space that separates the two classes. For a 3-feature dataset, this boundary is a 2D plane in 3D space. The model predicts class `1` for points on one side of the plane (where the sigmoid output â‰¥ 0.5) and class `0` for points on the other.

Because the dataset was generated with high class separability (`class_sep=2.0`), the decision boundary is expected to cleanly divide the two classes with minimal overlap. The effectiveness of this separation is reflected in the high accuracy, precision, and recall observed during evaluation.


## Understanding the Training Loss Curve

The training loss curve is a visual representation of how the model's error (or "cost") decreases over time as it learns from the data. In this project, the curve is generated by plotting the binary cross-entropy loss at each epoch during training.

### What It Shows

- **Y-axis (Cost):** Represents the loss value, which quantifies how far the model's predictions are from the actual labels.
- **X-axis (Epochs):** Represents the number of training iterations.

### Interpreting the Curve

- A **steep decline** at the beginning indicates rapid learning as the model adjusts its weights.
- A **gradual flattening** suggests the model is converging and making smaller updates.
- If the curve **plateaus**, the model has likely reached its optimal performance for the given learning rate and data.
- A **noisy or increasing curve** could indicate issues like a poor learning rate, overfitting, or unstable optimization.

### Uses

- Helps diagnose training behavior and convergence.
- Confirms whether the model is learning effectively.
- Can guide hyperparameter tuning (e.g., adjusting learning rate or epochs).


## Conclusion and Summary

This project successfully demonstrates the implementation of logistic regression from scratch using NumPy, applied to a synthetically generated binary classification dataset. By carefully designing the dataset with well-separated classes, the model was able to learn an effective decision boundary and achieve strong performance metrics.

###  Key Takeaways

- Built a custom logistic regression model without relying on machine learning libraries.
- Generated a clean, informative dataset using `sklearn.datasets.make_classification`.
- Visualized training progress through a loss curve to confirm convergence.
- Evaluated model performance using accuracy, precision, and recall.
- Interpreted learned coefficients to understand feature influence and decision logic.

This hands-on approach reinforces foundational concepts in supervised learning, optimization, and model evaluation, making it an excellent educational exercise for understanding how logistic regression works under the hood.
